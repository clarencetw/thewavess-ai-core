package services

import (
	"context"
	"fmt"
	"time"

	"github.com/clarencetw/thewavess-ai-core/utils"
	"github.com/openai/openai-go/v2"
	"github.com/openai/openai-go/v2/option"
	"github.com/openai/openai-go/v2/shared"
)

// MistralClient Mistral AI API å®¢æˆ¶ç«¯ (ä½¿ç”¨ OpenAI SDK)
type MistralClient struct {
	client      openai.Client
	model       string
	maxTokens   int
	temperature float64
	baseURL     string
}

// MistralRequest Mistral è«‹æ±‚çµæ§‹ (ç›¸å®¹ OpenAI æ ¼å¼)
type MistralRequest struct {
	Model       string           `json:"model"`
	Messages    []MistralMessage `json:"messages"`
	MaxTokens   int              `json:"max_tokens"`
	Temperature float64          `json:"temperature"`
	User        string           `json:"user,omitempty"`
}

// MistralMessage Mistral æ¶ˆæ¯çµæ§‹
type MistralMessage struct {
	Role    string `json:"role"` // system, user, assistant
	Content string `json:"content"`
}

// MistralResponse ä½¿ç”¨å®˜æ–¹ OpenAI SDK çš„ ChatCompletion ä½œç‚ºéŸ¿æ‡‰é¡å‹
type MistralResponse = openai.ChatCompletion

// NewMistralClient å‰µå»ºæ–°çš„ Mistral å®¢æˆ¶ç«¯ (ä½¿ç”¨ OpenAI SDK)
func NewMistralClient() *MistralClient {
	// ç¢ºä¿ç’°å¢ƒè®Šæ•¸å·²è¼‰å…¥
	utils.LoadEnv()

	apiKey := utils.GetEnvWithDefault("MISTRAL_API_KEY", "")
	if apiKey == "" {
		utils.Logger.Fatal("MISTRAL_API_KEY is required but not set in environment")
	}

	// å¾ç’°å¢ƒè®Šæ•¸è®€å–é…ç½®
	modelName := utils.GetEnvWithDefault("MISTRAL_MODEL", "mistral-medium-latest")
	maxTokens := utils.GetEnvIntWithDefault("MISTRAL_MAX_TOKENS", 1200)
	temperature := utils.GetEnvFloatWithDefault("MISTRAL_TEMPERATURE", 0.8)

	// ç²å– Mistral API URL
	baseURL := utils.GetEnvWithDefault("MISTRAL_API_URL", "https://api.mistral.ai/v1")

	// æº–å‚™å®¢æˆ¶ç«¯é¸é …
	options := []option.RequestOption{
		option.WithAPIKey(apiKey),
		option.WithBaseURL(baseURL),
	}

	// å‰µå»º OpenAI å®¢æˆ¶ç«¯ï¼Œä½¿ç”¨ Mistral ç«¯é»
	client := openai.NewClient(options...)

	utils.Logger.WithField("base_url", baseURL).Info("Using Mistral API with OpenAI SDK")

	return &MistralClient{
		client:      client,
		model:       modelName,
		maxTokens:   maxTokens,
		temperature: temperature,
		baseURL:     baseURL,
	}
}

// GenerateResponse ç”Ÿæˆå°è©±å›æ‡‰ (ä½¿ç”¨ OpenAI SDK + Structured Output)
func (c *MistralClient) GenerateResponse(ctx context.Context, request *MistralRequest) (*MistralResponse, error) {
	startTime := time.Now()

	// è¨˜éŒ„è«‹æ±‚é–‹å§‹
	utils.Logger.WithFields(map[string]interface{}{
		"service":        "mistral",
		"base_url":       c.baseURL,
		"model":          request.Model,
		"max_tokens":     request.MaxTokens,
		"temperature":    request.Temperature,
		"user":           request.User,
		"messages_count": len(request.Messages),
	}).Info("Mistral API request started")

	// é–‹ç™¼æ¨¡å¼ä¸‹è©³ç´°è¨˜éŒ„ prompt å…§å®¹
	if utils.GetEnvWithDefault("GO_ENV", "development") != "production" {
		utils.Logger.WithFields(map[string]interface{}{
			"service": "mistral",
			"model":   request.Model,
			"user":    request.User,
		}).Info("ğŸ¤– Mistral Request Details")

		for i, msg := range request.Messages {
			utils.Logger.WithFields(map[string]interface{}{
				"service":        "mistral",
				"message_index":  i,
				"role":           msg.Role,
				"content_length": len(msg.Content),
			}).Info(fmt.Sprintf("ğŸ“ Prompt [%s]: %s", msg.Role, msg.Content))
		}
	}

	// è¨­ç½®é»˜èªå€¼
	if request.Model == "" {
		request.Model = c.model
	}
	if request.MaxTokens == 0 {
		request.MaxTokens = c.maxTokens
	}
	if request.Temperature <= 0 {
		request.Temperature = c.temperature
	}

	// è½‰æ›ç‚º OpenAI SDK æ ¼å¼
	messages := make([]openai.ChatCompletionMessageParamUnion, len(request.Messages))
	for i, msg := range request.Messages {
		messages[i] = openai.UserMessage(msg.Content)
		switch msg.Role {
		case "system":
			messages[i] = openai.SystemMessage(msg.Content)
		case "assistant":
			messages[i] = openai.AssistantMessage(msg.Content)
		}
	}

	// æ§‹å»ºè«‹æ±‚åƒæ•¸
	params := openai.ChatCompletionNewParams{
		Model:       openai.ChatModel(request.Model),
		Messages:    messages,
		MaxTokens:   openai.Int(int64(request.MaxTokens)),
		Temperature: openai.Float(request.Temperature),
	}

	if request.User != "" {
		params.User = openai.String(request.User)
	}

	// è¨­ç½® Mistral çš„ JSON Schema (èˆ‡ OpenAI ç›¸åŒæ ¼å¼)
	schema := map[string]interface{}{
		"type": "object",
		"properties": map[string]interface{}{
			"content": map[string]interface{}{
				"type":        "string",
				"description": "è§’è‰²å›æ‡‰å…§å®¹ï¼ŒåŒ…å«å‹•ä½œæè¿°å’Œå°è©±",
			},
			"emotion_delta": map[string]interface{}{
				"type": "object",
				"properties": map[string]interface{}{
					"affection_change": map[string]interface{}{
						"type":        "integer",
						"description": "å¥½æ„Ÿåº¦è®ŠåŒ–ï¼Œå¿…é ˆæ˜¯æ•´æ•¸",
						"minimum":     -5,
						"maximum":     5,
					},
				},
				"required":             []string{"affection_change"},
				"additionalProperties": false,
			},
			"mood": map[string]interface{}{
				"type": "string",
				"enum": []string{
					"neutral", "happy", "excited", "shy", "romantic",
					"passionate", "pleased", "loving", "friendly",
					"polite", "concerned", "annoyed", "upset", "disappointed",
				},
				"description": "è§’è‰²ç•¶å‰æƒ…ç·’ç‹€æ…‹",
			},
			"relationship": map[string]interface{}{
				"type": "string",
				"enum": []string{"stranger", "friend", "close_friend", "lover", "soulmate"},
				"description": "è§’è‰²èˆ‡ç”¨æˆ¶çš„é—œä¿‚ç‹€æ…‹",
			},
			"intimacy_level": map[string]interface{}{
				"type": "string",
				"enum": []string{"distant", "friendly", "close", "intimate", "deeply_intimate"},
				"description": "è¦ªå¯†åº¦å±¤ç´š",
			},
			"reasoning": map[string]interface{}{
				"type":        "string",
				"description": "æ±ºç­–æ¨ç†èªªæ˜",
			},
		},
		"required":             []string{"content", "emotion_delta", "mood", "relationship", "intimacy_level", "reasoning"},
		"additionalProperties": false,
	}

	jsonSchemaParam := shared.ResponseFormatJSONSchemaJSONSchemaParam{
		Name:        "character_response",
		Description: openai.String("è§’è‰²å°è©±å›æ‡‰æ ¼å¼"),
		Schema:      schema,
		Strict:      openai.Bool(true),
	}

	params.ResponseFormat = openai.ChatCompletionNewParamsResponseFormatUnion{
		OfJSONSchema: &shared.ResponseFormatJSONSchemaParam{
			Type:       "json_schema",
			JSONSchema: jsonSchemaParam,
		},
	}

	// ç™¼é€è«‹æ±‚
	utils.Logger.WithFields(map[string]interface{}{
		"service": "mistral",
		"model":   request.Model,
	}).Info("Sending Mistral API request via OpenAI SDK")

	resp, err := c.client.Chat.Completions.New(ctx, params, option.WithRequestTimeout(60*time.Second))
	if err != nil {
		// è¨˜éŒ„è©³ç´°çš„éŒ¯èª¤ä¿¡æ¯ç”¨æ–¼è¨ºæ–·
		utils.Logger.WithFields(map[string]interface{}{
			"service":        "mistral",
			"error":          err.Error(),
			"error_type":     fmt.Sprintf("%T", err),
			"model":          request.Model,
			"base_url":       c.baseURL,
			"max_tokens":     request.MaxTokens,
			"temperature":    request.Temperature,
			"messages_count": len(request.Messages),
			"request_time":   time.Since(startTime),
		}).Error("Mistral API call failed")

		// æª¢æŸ¥æ˜¯å¦æ˜¯è¶…æ™‚éŒ¯èª¤
		if ctx.Err() == context.DeadlineExceeded {
			utils.Logger.WithFields(map[string]interface{}{
				"service":      "mistral",
				"timeout_type": "context_deadline",
				"elapsed":      time.Since(startTime),
			}).Error("Mistral API è«‹æ±‚è¶…æ™‚")
		}

		return nil, fmt.Errorf("failed Mistral API call: %w", err)
	}

	// è¨ˆç®—éŸ¿æ‡‰æ™‚é–“
	duration := time.Since(startTime)

	// è¨ˆç®— Mistral API æˆæœ¬ (å¤šæ¨¡å‹æ”¯æ´)
	promptTokens := int(resp.Usage.PromptTokens)
	completionTokens := int(resp.Usage.CompletionTokens)

	// Mistral å®šåƒ¹ç³»çµ± (per 1M tokens)
	var inputCostPer1M, outputCostPer1M float64

	switch string(resp.Model) {
	// Mistral Small series
	case "mistral-small-latest", "mistral-small-3.2", "mistral-small":
		inputCostPer1M = 0.10  // $0.10 per 1M input tokens
		outputCostPer1M = 0.30 // $0.30 per 1M output tokens

	// Mistral Medium series
	case "mistral-medium-latest", "mistral-medium-3", "mistral-medium":
		inputCostPer1M = 0.40  // $0.40 per 1M input tokens
		outputCostPer1M = 2.00 // $2.00 per 1M output tokens

	// Mistral Large series
	case "mistral-large-latest", "mistral-large", "mistral-large-2":
		inputCostPer1M = 2.00  // $2.00 per 1M input tokens
		outputCostPer1M = 6.00 // $6.00 per 1M output tokens

	// Magistral series (thinking models)
	case "magistral-small-latest", "magistral-small":
		inputCostPer1M = 0.50  // $0.50 per 1M input tokens
		outputCostPer1M = 1.50 // $1.50 per 1M output tokens

	case "magistral-medium-latest", "magistral-medium":
		inputCostPer1M = 2.00  // $2.00 per 1M input tokens
		outputCostPer1M = 5.00 // $5.00 per 1M output tokens

	// Legacy models
	case "mistral-7b-instruct", "mistral-8x7b-instruct":
		inputCostPer1M = 0.25  // Legacy pricing
		outputCostPer1M = 0.25

	default:
		// Default to Small pricing for unknown models
		inputCostPer1M = 0.10
		outputCostPer1M = 0.30
		utils.Logger.WithField("model", resp.Model).Warn("Unknown Mistral model, using Small pricing")
	}

	inputCost := float64(promptTokens) * inputCostPer1M / 1000000
	outputCost := float64(completionTokens) * outputCostPer1M / 1000000
	totalCost := inputCost + outputCost

	// è¨˜éŒ„æˆåŠŸéŸ¿æ‡‰ï¼ŒåŒ…å«è©³ç´°æˆæœ¬è³‡è¨Š
	utils.Logger.WithFields(map[string]interface{}{
		"service":            "mistral",
		"response_id":        resp.ID,
		"model":              resp.Model,
		"prompt_tokens":      resp.Usage.PromptTokens,
		"completion_tokens":  resp.Usage.CompletionTokens,
		"total_tokens":       resp.Usage.TotalTokens,
		"input_cost_usd":     fmt.Sprintf("$%.6f", inputCost),
		"output_cost_usd":    fmt.Sprintf("$%.6f", outputCost),
		"total_cost_usd":     fmt.Sprintf("$%.6f", totalCost),
		"input_rate_per_1m":  fmt.Sprintf("$%.2f", inputCostPer1M),
		"output_rate_per_1m": fmt.Sprintf("$%.2f", outputCostPer1M),
		"choices_count":      len(resp.Choices),
		"duration_ms":        duration.Milliseconds(),
	}).Info("Mistral API response received")

	// é–‹ç™¼æ¨¡å¼ä¸‹è©³ç´°è¨˜éŒ„éŸ¿æ‡‰å…§å®¹
	if utils.GetEnvWithDefault("GO_ENV", "development") != "production" {
		utils.Logger.WithFields(map[string]interface{}{
			"service":     "mistral",
			"response_id": resp.ID,
			"model":       resp.Model,
		}).Info("ğŸ¯ Mistral Response Details")

		for i, choice := range resp.Choices {
			utils.Logger.WithFields(map[string]interface{}{
				"service":        "mistral",
				"choice_index":   i,
				"finish_reason":  string(choice.FinishReason),
				"content_length": len(choice.Message.Content),
			}).Info(fmt.Sprintf("ğŸ’¬ Response [%d]: %s", i, choice.Message.Content))
		}
	}

	return resp, nil
}