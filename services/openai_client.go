package services

import (
	"context"
	"fmt"
	"log"
	"os"
	"strings"
	"time"

	"github.com/clarencetw/thewavess-ai-core/utils"
	"github.com/openai/openai-go/v2"
	"github.com/openai/openai-go/v2/option"
)

// OpenAIClient OpenAI å®¢æˆ¶ç«¯
type OpenAIClient struct {
	client      openai.Client
	model       openai.ChatModel
	maxTokens   int
	temperature float64
	baseURL     string
}

// OpenAIRequest OpenAI è«‹æ±‚çµæ§‹
type OpenAIRequest struct {
	Model       string          `json:"model"`
	Messages    []OpenAIMessage `json:"messages"`
	MaxTokens   int             `json:"max_tokens"`
	Temperature float64         `json:"temperature"`
	User        string          `json:"user,omitempty"`
}

// OpenAIMessage OpenAI æ¶ˆæ¯çµæ§‹
type OpenAIMessage struct {
	Role    string `json:"role"` // system, user, assistant
	Content string `json:"content"`
}

// OpenAIResponse ä½¿ç”¨å®˜æ–¹ SDK çš„ ChatCompletion ä½œç‚ºéŸ¿æ‡‰é¡å‹
type OpenAIResponse = openai.ChatCompletion

// NewOpenAIClient å‰µå»ºæ–°çš„ OpenAI å®¢æˆ¶ç«¯
func NewOpenAIClient() *OpenAIClient {
	// ç¢ºä¿ç’°å¢ƒè®Šæ•¸å·²è¼‰å…¥
	utils.LoadEnv()

	apiKey := utils.GetEnvWithDefault("OPENAI_API_KEY", "")
	if apiKey == "" {
		utils.Logger.Fatal("OPENAI_API_KEY is required but not set in environment")
	}

	// å¾ç’°å¢ƒè®Šæ•¸è®€å–é…ç½®ï¼Œæä¾›é è¨­å€¼
	modelName := utils.GetEnvWithDefault("OPENAI_MODEL", "gpt-4o-mini")
	maxTokens := utils.GetEnvIntWithDefault("OPENAI_MAX_TOKENS", 1200)
	temperature := utils.GetEnvFloatWithDefault("OPENAI_TEMPERATURE", 0.8)

	// ç²å–è‡ªå®šç¾© API URL
	baseURL := utils.GetEnvWithDefault("OPENAI_API_URL", "https://api.openai.com/v1")

	// è¨­å®š model
	var model openai.ChatModel
	switch modelName {
	case "gpt-4o":
		model = openai.ChatModelGPT4o
	case "gpt-4o-mini":
		model = openai.ChatModelGPT4oMini
	case "gpt-4":
		model = openai.ChatModelGPT4
	case "gpt-3.5-turbo":
		model = openai.ChatModelGPT3_5Turbo
	default:
		model = openai.ChatModelGPT4oMini
	}

	// æº–å‚™å®¢æˆ¶ç«¯é¸é …
	options := []option.RequestOption{
		option.WithAPIKey(apiKey),
	}

	// æ·»åŠ è‡ªå®šç¾©ç«¯é»
	if baseURL != "https://api.openai.com/v1" {
		options = append(options, option.WithBaseURL(baseURL))
		utils.Logger.WithField("base_url", baseURL).Info("Using custom OpenAI API URL")
	}

	// åœ¨é–‹ç™¼ç’°å¢ƒä¸‹å•Ÿç”¨ debug æ—¥èªŒ
	if utils.GetEnvWithDefault("GO_ENV", "development") != "production" {
		debugLogger := log.New(os.Stderr, "[OpenAI-DEBUG] ", log.LstdFlags)
		options = append(options, option.WithDebugLog(debugLogger))
		utils.Logger.Info("OpenAI SDK debug logging enabled")
	}

	client := openai.NewClient(options...)

	return &OpenAIClient{
		client:      client,
		model:       model,
		maxTokens:   maxTokens,
		temperature: temperature,
		baseURL:     baseURL,
	}
}

// GenerateResponse ç”Ÿæˆå°è©±å›æ‡‰
func (c *OpenAIClient) GenerateResponse(ctx context.Context, request *OpenAIRequest) (*OpenAIResponse, error) {
	// è¨˜éŒ„è«‹æ±‚é–‹å§‹
	utils.Logger.WithFields(map[string]interface{}{
		"service":        "openai",
		"base_url":       c.baseURL,
		"model":          c.model,
		"max_tokens":     c.maxTokens,
		"temperature":    c.temperature,
		"user":           request.User,
		"messages_count": len(request.Messages),
	}).Info("OpenAI API request started")

	// é–‹ç™¼æ¨¡å¼ä¸‹è©³ç´°è¨˜éŒ„ prompt å…§å®¹
	if utils.GetEnvWithDefault("GO_ENV", "development") != "production" {
		utils.Logger.WithFields(map[string]interface{}{
			"service": "openai",
			"model":   c.model,
			"user":    request.User,
		}).Info("ğŸ¤– OpenAI Request Details")

		for i, msg := range request.Messages {
			utils.Logger.WithFields(map[string]interface{}{
				"service":        "openai",
				"message_index":  i,
				"role":           msg.Role,
				"content_length": len(msg.Content),
			}).Info(fmt.Sprintf("ğŸ“ Prompt [%s]: %s", strings.ToUpper(msg.Role), msg.Content))
		}
	} else {
		// ç”Ÿç”¢ç’°å¢ƒåªè¨˜éŒ„åŸºæœ¬ä¿¡æ¯
		for i, msg := range request.Messages {
			utils.Logger.WithFields(map[string]interface{}{
				"service":        "openai",
				"message_index":  i,
				"role":           msg.Role,
				"content_length": len(msg.Content),
			}).Debug("OpenAI request message")
		}
	}

	// è½‰æ›æ¶ˆæ¯æ ¼å¼
	messages := make([]openai.ChatCompletionMessageParamUnion, len(request.Messages))
	for i, msg := range request.Messages {
		switch msg.Role {
		case "system":
			messages[i] = openai.SystemMessage(msg.Content)
		case "user":
			messages[i] = openai.UserMessage(msg.Content)
		case "assistant":
			messages[i] = openai.AssistantMessage(msg.Content)
		default:
			messages[i] = openai.UserMessage(msg.Content)
		}
	}

	// å»ºç«‹ API åƒæ•¸
	params := openai.ChatCompletionNewParams{
		Model:       c.model,
		Messages:    messages,
		MaxTokens:   openai.Int(int64(c.maxTokens)),
		Temperature: openai.Float(c.temperature),
		User:        openai.String(request.User),
	}

	// å¯é¸åŠŸèƒ½ï¼šLogprobsï¼ˆèª¿è©¦å’Œåˆ†ææ¨¡å‹ä¿¡å¿ƒï¼‰
	if utils.GetEnvWithDefault("OPENAI_LOGPROBS", "false") == "true" {
		params.Logprobs = openai.Bool(true)
		if topLogprobs := utils.GetEnvIntWithDefault("OPENAI_TOP_LOGPROBS", 0); topLogprobs > 0 && topLogprobs <= 20 {
			params.TopLogprobs = openai.Int(int64(topLogprobs))
		}
	}

	// å¯é¸åŠŸèƒ½ï¼šæœå‹™å±¤ç´šæ§åˆ¶
	if serviceTier := utils.GetEnvWithDefault("OPENAI_SERVICE_TIER", ""); serviceTier != "" {
		switch serviceTier {
		case "auto", "default", "flex", "scale", "priority":
			params.ServiceTier = openai.ChatCompletionNewParamsServiceTier(serviceTier)
		}
	}

	// åŠ å…¥ç¨®å­åƒæ•¸ä»¥æé«˜ä¸€è‡´æ€§ï¼ˆå¯é¸ï¼‰
	if seed := utils.GetEnvWithDefault("OPENAI_SEED", ""); seed != "" {
		if seedInt := utils.GetEnvIntWithDefault("OPENAI_SEED", 0); seedInt > 0 {
			params.Seed = openai.Int(int64(seedInt))
		}
	}

	// èª¿ç”¨ OpenAI API
	startTime := time.Now()
	resp, err := c.client.Chat.Completions.New(ctx, params, option.WithRequestTimeout(30*time.Second))

	if err != nil {
		// è¨˜éŒ„è©³ç´°çš„éŒ¯èª¤ä¿¡æ¯ç”¨æ–¼è¨ºæ–·
		utils.Logger.WithFields(map[string]interface{}{
			"service":        "openai",
			"error":          err.Error(),
			"error_type":     fmt.Sprintf("%T", err),
			"model":          string(c.model),
			"user":           request.User,
			"base_url":       c.baseURL,
			"max_tokens":     request.MaxTokens,
			"temperature":    request.Temperature,
			"messages_count": len(request.Messages),
			"request_time":   time.Since(startTime),
		}).Error("OpenAI API call failed")

		// æª¢æŸ¥æ˜¯å¦æ˜¯è¶…æ™‚éŒ¯èª¤
		if ctx.Err() == context.DeadlineExceeded {
			utils.Logger.WithFields(map[string]interface{}{
				"service":      "openai",
				"timeout_type": "context_deadline",
				"elapsed":      time.Since(startTime),
			}).Error("OpenAI API è«‹æ±‚è¶…æ™‚")
		}

		return nil, fmt.Errorf("failed OpenAI API call: %w", err)
	}

	// è¨ˆç®—æº–ç¢ºæˆæœ¬ - åˆ†åˆ¥è¨ˆç®— input å’Œ output token æˆæœ¬
	promptTokens := int(resp.Usage.PromptTokens)
	completionTokens := int(resp.Usage.CompletionTokens)

	var inputCostPer1K, outputCostPer1K float64
	switch string(resp.Model) {
	// GPT-5 series (Standard tier)
	case "gpt-5", "gpt-5-chat-latest":
		inputCostPer1K = 0.00125 // $1.25 per 1M tokens = $0.00125 per 1K tokens
		outputCostPer1K = 0.01   // $10.00 per 1M tokens = $0.01 per 1K tokens
	case "gpt-5-mini":
		inputCostPer1K = 0.00025 // $0.25 per 1M tokens = $0.00025 per 1K tokens
		outputCostPer1K = 0.002  // $2.00 per 1M tokens = $0.002 per 1K tokens
	case "gpt-5-nano":
		inputCostPer1K = 0.00005 // $0.05 per 1M tokens = $0.00005 per 1K tokens
		outputCostPer1K = 0.0004 // $0.40 per 1M tokens = $0.0004 per 1K tokens
	// GPT-4.1 series (Standard tier)
	case "gpt-4.1":
		inputCostPer1K = 0.002  // $2.00 per 1M tokens = $0.002 per 1K tokens
		outputCostPer1K = 0.008 // $8.00 per 1M tokens = $0.008 per 1K tokens
	case "gpt-4.1-mini":
		inputCostPer1K = 0.0004  // $0.40 per 1M tokens = $0.0004 per 1K tokens
		outputCostPer1K = 0.0016 // $1.60 per 1M tokens = $0.0016 per 1K tokens
	case "gpt-4.1-nano":
		inputCostPer1K = 0.0001  // $0.10 per 1M tokens = $0.0001 per 1K tokens
		outputCostPer1K = 0.0004 // $0.40 per 1M tokens = $0.0004 per 1K tokens
	// O-series models (Standard tier)
	case "o1":
		inputCostPer1K = 0.015 // $15.00 per 1M tokens = $0.015 per 1K tokens
		outputCostPer1K = 0.06 // $60.00 per 1M tokens = $0.06 per 1K tokens
	case "o1-pro":
		inputCostPer1K = 0.15 // $150.00 per 1M tokens = $0.15 per 1K tokens
		outputCostPer1K = 0.6 // $600.00 per 1M tokens = $0.6 per 1K tokens
	case "o1-mini":
		inputCostPer1K = 0.0011  // $1.10 per 1M tokens = $0.0011 per 1K tokens
		outputCostPer1K = 0.0044 // $4.40 per 1M tokens = $0.0044 per 1K tokens
	case "o3", "o3-pro", "o3-mini", "o3-deep-research":
		// Use o3 pricing for all o3 variants
		inputCostPer1K = 0.002  // $2.00 per 1M tokens = $0.002 per 1K tokens
		outputCostPer1K = 0.008 // $8.00 per 1M tokens = $0.008 per 1K tokens
	case "o4-mini", "o4-mini-deep-research":
		inputCostPer1K = 0.0011  // $1.10 per 1M tokens = $0.0011 per 1K tokens
		outputCostPer1K = 0.0044 // $4.40 per 1M tokens = $0.0044 per 1K tokens
	// Existing GPT-4o series
	case "gpt-4o":
		inputCostPer1K = 0.0025 // $2.50 per 1M tokens = $0.0025 per 1K tokens (Standard tier)
		outputCostPer1K = 0.01  // $10.00 per 1M tokens = $0.01 per 1K tokens
	case "gpt-4o-mini":
		inputCostPer1K = 0.00015 // $0.15 per 1M tokens = $0.00015 per 1K tokens (Standard tier)
		outputCostPer1K = 0.0006 // $0.60 per 1M tokens = $0.0006 per 1K tokens
	case "gpt-4", "gpt-4-0613", "gpt-4-0314":
		inputCostPer1K = 0.03  // $30.00 per 1M tokens = $0.03 per 1K tokens (Standard tier)
		outputCostPer1K = 0.06 // $60.00 per 1M tokens = $0.06 per 1K tokens
	case "gpt-3.5-turbo", "gpt-3.5-turbo-0125":
		inputCostPer1K = 0.0005  // $0.50 per 1M tokens = $0.0005 per 1K tokens (Standard tier)
		outputCostPer1K = 0.0015 // $1.50 per 1M tokens = $0.0015 per 1K tokens
	case "gpt-4-turbo", "gpt-4-turbo-2024-04-09":
		inputCostPer1K = 0.01  // $10.00 per 1M tokens = $0.01 per 1K tokens (Standard tier)
		outputCostPer1K = 0.03 // $30.00 per 1M tokens = $0.03 per 1K tokens
	default:
		inputCostPer1K = 0.001  // Default input estimate
		outputCostPer1K = 0.002 // Default output estimate
	}

	inputCost := float64(promptTokens) * inputCostPer1K / 1000
	outputCost := float64(completionTokens) * outputCostPer1K / 1000
	costEstimate := inputCost + outputCost

	// è¨˜éŒ„APIéŸ¿æ‡‰ä¿¡æ¯ï¼ŒåŒ…å«è©³ç´°çš„ token ä½¿ç”¨å’Œæˆæœ¬åˆ†è§£
	logFields := map[string]interface{}{
		"service":            "openai",
		"response_id":        resp.ID,
		"model":              string(resp.Model),
		"object":             string(resp.Object),
		"created":            resp.Created,
		"prompt_tokens":      resp.Usage.PromptTokens,
		"completion_tokens":  resp.Usage.CompletionTokens,
		"total_tokens":       resp.Usage.TotalTokens,
		"input_cost_usd":     fmt.Sprintf("$%.6f", inputCost),
		"output_cost_usd":    fmt.Sprintf("$%.6f", outputCost),
		"total_cost_usd":     fmt.Sprintf("$%.6f", costEstimate),
		"input_rate_per_1k":  fmt.Sprintf("$%.6f", inputCostPer1K),
		"output_rate_per_1k": fmt.Sprintf("$%.6f", outputCostPer1K),
		"choices_count":      len(resp.Choices),
	}

	// åŠ å…¥ finish_reason å’Œå…§å®¹éæ¿¾ç›¸é—œè³‡è¨Š
	if len(resp.Choices) > 0 {
		finishReason := string(resp.Choices[0].FinishReason)
		logFields["finish_reason"] = finishReason

		// æ¨™è¨˜æ˜¯å¦è¢«å…§å®¹éæ¿¾å™¨é˜»æ“‹
		if finishReason == "content_filter" {
			logFields["content_filtered"] = true
		}
	}

	// SystemFingerprint å·²è¢«å®˜æ–¹æ¨™è¨˜ç‚º deprecatedï¼Œä¸å†è¨˜éŒ„

	// åŠ å…¥æœå‹™å±¤ç´šè³‡è¨Šï¼ˆå¯èƒ½å½±éŸ¿å…§å®¹éæ¿¾ï¼‰
	if resp.ServiceTier != "" {
		logFields["service_tier"] = string(resp.ServiceTier)
	}

	// è¨˜éŒ„ Logprobs è³‡è¨Šï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
	if len(resp.Choices) > 0 {
		logprobs := resp.Choices[0].Logprobs
		if logprobs.Content != nil && len(logprobs.Content) > 0 {
			logFields["logprobs_enabled"] = true
			logFields["logprobs_tokens"] = len(logprobs.Content)
		}
	}

	// åŠ å…¥ seed åƒæ•¸ï¼ˆå¦‚æœæœ‰è¨­å®šï¼‰
	if seed := utils.GetEnvWithDefault("OPENAI_SEED", ""); seed != "" {
		logFields["seed_used"] = seed
	}

	utils.Logger.WithFields(logFields).Info("OpenAI API response received")

	// é–‹ç™¼æ¨¡å¼ä¸‹è©³ç´°è¨˜éŒ„éŸ¿æ‡‰å…§å®¹
	if utils.GetEnvWithDefault("GO_ENV", "development") != "production" {
		utils.Logger.WithFields(map[string]interface{}{
			"service":     "openai",
			"response_id": resp.ID,
			"model":       string(resp.Model),
		}).Info("ğŸ¯ OpenAI Response Details")

		for i, choice := range resp.Choices {
			utils.Logger.WithFields(map[string]interface{}{
				"service":        "openai",
				"choice_index":   i,
				"finish_reason":  string(choice.FinishReason),
				"content_length": len(choice.Message.Content),
			}).Info(fmt.Sprintf("ğŸ’¬ Response [%d]: %s", i, choice.Message.Content))
		}
	} else {
		// ç”Ÿç”¢ç’°å¢ƒåªè¨˜éŒ„åŸºæœ¬ä¿¡æ¯
		for i, choice := range resp.Choices {
			utils.Logger.WithFields(map[string]interface{}{
				"service":        "openai",
				"choice_index":   i,
				"finish_reason":  string(choice.FinishReason),
				"content_length": len(choice.Message.Content),
			}).Debug("OpenAI response choice")
		}
	}

	// ç›´æ¥è¿”å›å®˜æ–¹ SDK çš„éŸ¿æ‡‰çµæ§‹
	return resp, nil
}

